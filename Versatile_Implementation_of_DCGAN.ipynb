{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Root directory for dataset\n",
    "dataroot = \"C:/Users/Abhishek Mukherjee/Documents/DCGANs/Data\"\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 32\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "# Creating the transformations\n",
    "transform = transforms.Compose([transforms.Scale(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]) # We create a list of transformations (scaling, tensor conversion, normalization) to apply to the input images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset for image corpus\n",
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the dataset for tensor images\n",
    "##Commented while using image corpus\n",
    "#dataset = dset.CIFAR10(root = 'C:/Users/Abhishek Mukherjee/Downloads/Module 3 - GANs/data/cifar-10-batches-py', download = True, transform = transform) # We download the training set in the ./data folder and we apply the previous transformations on each image.\n",
    "#dataloader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle = True, num_workers =workers) # We use dataLoader to get the images of the training set batch by batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loses vector\n",
    "lossG=[]\n",
    "lossD=[]\n",
    "\n",
    "# Defining the weights_init function that takes as input a neural network m and that will initialize all its weights.\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "# Defining the generator\n",
    "class G(nn.Module):\n",
    "\n",
    "    def __init__(self): \n",
    "        super(G, self).__init__() \n",
    "        self.main = nn.Sequential( \n",
    "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False), \n",
    "            nn.BatchNorm2d(512), \n",
    "            nn.ReLU(True), \n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False), \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True), \n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False), \n",
    "            nn.BatchNorm2d(128), \n",
    "            nn.ReLU(True), \n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(64), \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False), \n",
    "            nn.Tanh() \n",
    "        )\n",
    "\n",
    "    def forward(self, input): \n",
    "        output = self.main(input) \n",
    "        return output \n",
    "\n",
    "# Creating the generator\n",
    "netG = G() # We create the generator object.\n",
    "netG.apply(weights_init) # We initialize all the weights of its neural network.\n",
    "\n",
    "# Defining the discriminator\n",
    "\n",
    "class D(nn.Module): # We introduce a class to define the discriminator.\n",
    "\n",
    "    def __init__(self): \n",
    "        super(D, self).__init__() \n",
    "        self.main = nn.Sequential( \n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias = False), # We start with a convolution.\n",
    "            nn.LeakyReLU(0.2, inplace = True), # We apply a LeakyReLU.\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias = False), # We add another convolution.\n",
    "            nn.BatchNorm2d(128), # We normalize all the features along the dimension of the batch.\n",
    "            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias = False), # We add another convolution.\n",
    "            nn.BatchNorm2d(256), # We normalize again.\n",
    "            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias = False), # We add another convolution.\n",
    "            nn.BatchNorm2d(512), # We normalize again.\n",
    "            nn.LeakyReLU(0.2, inplace = True), # We apply another LeakyReLU.\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias = False), # We add another convolution.\n",
    "            nn.Sigmoid() # We apply a Sigmoid rectification to break the linearity and stay between 0 and 1.\n",
    "        )\n",
    "\n",
    "    def forward(self, input): \n",
    "        output = self.main(input) \n",
    "        return output.view(-1) \n",
    "\n",
    "# Creating the discriminator\n",
    "netD = D() \n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Training the DCGANs\n",
    "\n",
    "criterion = nn.BCELoss() \n",
    "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999)) \n",
    "optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Num. of epochs: \n",
    "totEpochs=35\n",
    "\n",
    "for epoch in range(totEpochs): \n",
    "\n",
    "    for i, data in enumerate(dataloader, 0): \n",
    "        \n",
    "        #Updating the weights of the neural network of the discriminator\n",
    "        netD.zero_grad() \n",
    "        \n",
    "        # Training the discriminator with a real image of the dataset\n",
    "        real, _ = data \n",
    "        input = Variable(real) \n",
    "        target = Variable(torch.ones(input.size()[0]))\n",
    "        output = netD(input) \n",
    "        errD_real = criterion(output, target) \n",
    "        \n",
    "        # Training the discriminator with a fake image generated by the generator\n",
    "        noise = Variable(torch.randn(input.size()[0], 100, 1, 1)) \n",
    "        fake = netG(noise) \n",
    "        errD_fake = criterion(output, target) \n",
    "\n",
    "        # Backpropagating the total error\n",
    "        errD = errD_real + errD_fake \n",
    "        errD.backward()\n",
    "        optimizerD.step() \n",
    "\n",
    "        # Updating the weights of the neural network of the generator\n",
    "        netG.zero_grad() \n",
    "        target = Variable(torch.ones(input.size()[0])) \n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, target)\n",
    "        errG.backward() \n",
    "        optimizerG.step() \n",
    "        \n",
    "        #Printing the losses and saving the real images and the generated images of the minibatch every 100 steps\n",
    "        print(f'current epoch is {epoch}, value of the image number in data-loader {i} \\n Discriminator loss is {errD.data.item()}, Generator loss is {errG.data.item()}')\n",
    "        \n",
    "        #Appending teh losses to plot loss at the end\n",
    "        lossG.append(errG.item())\n",
    "        lossD.append(errD.item())\n",
    "        \n",
    "        if i % 100 == 0: # Every 100 steps:\n",
    "            vutils.save_image(real, '%s/real_samples.png' % \"C:/Users/Abhishek Mukherjee/Documents/DCGANs/results\", normalize = True) # We save the real images of the minibatch.\n",
    "            fake = netG(noise) \n",
    "            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"C:/Users/Abhishek Mukherjee/Documents/DCGANs/results\", epoch), normalize = True) # We also save the fake generated images of the minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
